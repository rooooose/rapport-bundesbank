\section{Déroulement du travail}

\subsection{Méthodologie et Organisation}

Durant toute la durée du stage, il y avait un suivi régulier, permettant une gestion de projet selon la méthode agile.

Chaque jour, une réunion d'alignement de 30 min s'appelant "Jour Fixe" avec mon équipe (dont mon tuteur), le SFDH, avait lieu.
Cela permettait à chaque membre de tenir les autres au courant de ce qu'il a fait la veille et de se faire aider par l'équipe en cas de questions.

Au début de chaque mois se déroulait additionnellement le Jour Fixe avec Stefan Bender, le responsable du département DSZ, afin d'aborder des points organisationnels et le tenir au courant du travail général de l'équipe du SFDH.

Un Jour Fixe DSZ se tenait tous les 3èmes mercredis du mois, avec l'ensemble du département, pour un alignement général des différents sous-départements. 

Au milieu du stage, un "Feedback Call" avec mon tuteur était l'occasion de partager mon ressenti sur le stage, ce que je voulais faire pendant les mois restants, si les projets précédents m'avaient plu, etc... 
Cela permettait aussi à mon tuteur de me donner son avis sur mon travail, et d'éventuellement m'indiquer les points à améliorer.
\\

Comme le travail avait lieu en hybride, toutes les réunions se faisaient en visio via Webex.
J'ai d'ailleurs bénéficié d'un ordinateur portable afin de pouvoir faire du télétravail plusieurs jours par semaine.
La communication avec les membres de la Bundesbank se faisait alors via la messagerie Jabber et par mail.
\\

Au niveau de l'organisation concernant purement mes tâches, le travail m'était toujours donné tâche par tâche, ce qui me permettait de ne pas être débordée et d'être aussi plus efficace.
De plus, cela encourage des réunions fréquentes et ainsi une gestion de projet agile.

Dès qu'une tâche était terminée, j'organisais une réunion d'avancement avec les personnes concernées pour avoir leur feedback ainsi que de nouvelles tâches.

\paragraph{Sécurité informatique}

S'agissant d'une grande banque centrale, la sécurité est bien sûr prioritaire. 
Les ordinateurs ne peuvent être connectés à internet seulement lorsqu'ils sont connectés au réseau de la Banque (directement ou par VPN), et seuls certains sites sont autorisés.

La Banque possède un serveur interne dans lequel tous les dossiers et fichiers sont partagés. 
Chaque dossier est protégé et n'est accessible qu'à certains groupes.

Si un nouveau logiciel a besoin d'être installé, il doit être commandé via la plateforme Servity, puis l'installation doit être déclenchée par le service IT.
De même pour certains accès, notamment pour Gitlab ou Python, qui sont à demander sur la plateforme BIAM.
\\

La mise en place de Python comprend notamment la création d'environnements virtuels pour chaque projet, afin d'éviter des problèmes aux changements de version de Python.

Heureusement, tout cela est très bien guidé sur Confluence, une plateforme de documentation partagée, où sont notamment disponibles des guides pour la mise en place de Python sur les machines de la banque.
Elle est également utilisée pour documenter certains projets avec l'état des lieux actuel, les recherches déjà effectuées, les étapes suivantes, etc...

% \pagebreak
\subsection{Application de la méthode et Résultats}

\subsubsection{Projet Gaia}

La toute première étape du projet était de participer à une réunion avec Manuel Fangmann, un membre du pôle Innovation de la Bundesbank, qui participait activement au projet durant l'ensemble de mon stage.
Le projet m'a donc été présenté et ma 1ère mission attribuée : Ecrire un code python qui parcourt les résultats Google avec comme entrée 'companyName + "sustainability reports" + year + "pdf"' et télécharger tous les pdf.

La liste d'entreprise est dans un premier temps celle du DAX40, qui réunit les 30 plus grandes entreprises cotées à la Bourse de Francfort.
J'ai décidé de tester la recherche avec une liste d'années allant de 2017 à 2022.

Il m'a été demandé de travailler sur mon ordinateur personnel pour ce projet, car étant donné que l'accès à la quasi-totalité des sites est bloqué sur les machine de la banque, il serait compliqué de téléchargé les pdfs depuis ces sites.
\\

En 2 jours, j'ai donc implémenté le Google Crawler à l'aide du package requests-html, et téléchargé les pdfs les plus pertinents, mais les résultats n'étaient pas des meilleurs. 
En effet, les résultats de recherche n'incluaient pas que des pdfs, et ceux trouvés n'étaient donc parfois pas les plus pertinents.
\\

J'ai ensuite tenté de récupérer les titres des PDFs ou des liens tels qu'ils sont indiqués pour les utilisateurs afin de pouvoir y lire le nom de l'entreprise et l'année, mais je n'ai pas réussi et cette entreprise était dans tous les cas inutile, puisque ces informations pouvaient souvent se trouver directement dans le lien.

Dans ma recherche d'amélioration, j'ai trouvé un package donnant des résultats plus pertinents : Beautiful Soup.
J'ai également mis en place un comptage des liens "douteux", c'est-à-dire ceux ne contenant ni le nom de la bonne entreprise, ni l'année.
Les différents liens sont désormais triés à la volée selon s'ils sont douteux ou non, et ces 2 listes ensuite sauvegardés dans le fichier json correspondant.
\\

Après la 1ère réunion d'avancement, j'ai mis en place beaucoup d'améliorations à mon code :
\begin{itemize}
    \item Calcul du pourcentage de pdfs justes trouvés par an
    \item Amélioration du tri des pdfs trouvés/non trouvés : Désormais, un pdf est défini comme douteux s'il ne contient pas l'année ET/OU pas le nom de l'entreprise, et non pas uniquement si aucun des deux n'est présent.
    \item Ajout d'un opérateur de recherche avancée pour n'obtenir que les les liens qui menant à un pdf (filetype:pdf) (-> il n'y a donc plus de rapports "not found", seulement des pdfs douteux)
    \item Parfois, seuls les 2 derniers chiffres de l'année sont notés dans le lien, ou encore l'une des deux parties du nom de la société (Ex: "Telekom" dans "Deutsche Telekom"), j'en ai donc tenu compte
    \item Un problème qui revenait assez fréquemment était que la bonne année pouvait être présente dans le lien, mais que dans le nom du fichier, une autre année était indiquée. Afin de passer en priorité l'année présente dans le nom du fichier, j'ai simplement vérifié que l'année était inscrite précisément à cet endroit, sans inspecter le reste du lien.
    \item les PDFs ne peuvent être définis comme trouvés uniquement si le lien contient le terme "Report"
\end{itemize}

La principale difficulté lors de la mise en place de ces améliorations était la gestion d'une condition très longue pour la définition d'un lien "non trouvé", qui est d'autant plus négative, avec à la fois des OR et AND.
J'ai finalement simplifié le résonnement en faisant un condition positive pour les liens "trouvés".
\\

Pour anticiper la suite du projet avec l'étape de Text Mining des PDFs, il m'a été demandé de tester l'extraction de textes et d'images des PDFs, ce que j'ai réussi sans difficulté avec le package Fitz.
\\

Concernant le téléchargement des PDFs, j'avais des difficultés à tous les télécharger, certains sites bloquant les requêtes lorsqu'elles sont détectées comme étant automatisées.
Cela se traduisait soit par une requête n'aboutissant jamais mais sans erreur, soit par un code de réponse "403-Forbidden".
Un autre cas d'impossibilité du téléchargement est lorsque les rapports sont incorporés à la page web, avec un "viewer intégré", ou encore lorsque le pdf n'est plus disponible sur la page en question.

J'ai pu résoudre le cas de requête infinie grâce à une liste de "User Agents" qui seraient choisis à chaque fois aléatoirement pour être inclus dans le header de la requête.
L'erreur 403 n'était en revanche pas solvable, même en tentant avec un Pool de Proxys. 
\\

Lors d'une réunion suivante, l'idée de lire les PDFs afin de mieux les classer a été suggérée. 
Je me suis donc attelée à cette tâche et ai extrait le texte des 3 premières pages des rapports dont le lien est douteux afin d'y chercher l'année, le nom de l'entreprise (ou partie du nom) et le terme "Sustainable Report" ou un dérivé.
Le lien était donc reclassé en conséquence.

Cela a considérablement amélioré les résultats, car ne considérer que le lien était très réducteur, et beaucoup de rapports douteux étaient en réalité justes.
Mais seules les 3 premières pages étaient optimales à lire, car après une autre année pouvait apparaître, bien qu'il s'agissait du rapport d'une autre année, faussant les résultats.
Lire moins de 3 pages était en revanche trop peu pour trouver toutes les informations requises.

Cette lecture de PDFs n'a cependant pas résolu tous les problèmes, bien qu'ils soient rares :
\begin{itemize}
    \item Le nom de l'entreprise ou même l'année ne sont parfois inscrits que sous forme d'image, non reconnue dans l'extraction de texte.
    \item L'année recherchée peut être écrite au début du rapport sans qu'il ne s'agisse du rapport de cette année-là. Celui-ci est donc noté comme trouvé alors qu'il est faux.
\end{itemize}

Après la prochaine réunion est venue l'étape de Refactoring du code, afin de 1-Rendre le code qui devenait de plus en plus fourni, plus lisible. 
2- Rendre les différentes étapes indépendantes les unes des autres.

Mon code pouvait être découpé en 5 parties : Préparation de la liste d'entrperises, Crawling (=recherche des liens), Téléchargement des pdfs, Lecture des fichiers, et écriture des statistiques finales des résultats trouvés.
Il y a donc 1 fichier par étape, en plus du Main qui les appelle toutes. 

Pour que toutes ces étapes soient plus indépendantes les unes des autres, il fallait que le tri des liens se fasse en deux temps : une première fois en même temps que le crawling, et une deuxième fois après la lecture automatique des rapports douteux.
J'ai donc 4 fichiers de résultats où se trouvent les différentes listes des liens obtenus pour chaque requête : found-list et doubt-list avant le téléchargement, et de même après le téléchargement.
Le fichier found-list après téléchargement est composé de la copie de la 1ère found-list, mais avec en plus de nouveaux liens qui étaient au départ douteux.
\\

La procédure est détaillée plus clairement ici :
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{procedure.png}
    \caption{Procédure du code}
\end{figure}

Une fois que tout fonctionnait bien, il était temps de tester le code sur un volume beaucoup plus grand d'entreprises, à savoir la liste de celles du MSCI World, Index boursier pour les plus grandes entreprises dans le monde entier.
Cette liste en contient 3425 différentes. En ajoutant celles du DAX40, j'ai un échantillon de 3455 entreprises. 
Néanmoins, le fichier csv fourni contenait des doublons, et les noms des entreprises étaient parfois beaucoup trop longs (Ex : CONTEXTLOGIC INC CLASS A).
J'ai donc dans un premier temps fait une liste des termes "interdits" et ai tronqué les noms à partir de ces termes pour rendre une liste de noms propre (notamment "inc", "group", "plc", "class", etc.).

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{msci.png}
    \caption{Extrait de la liste MSCI avec les noms des entreprises en jaune}
\end{figure}

Pour ce projet, nous bénéficions d'une instance AWS Guacamole afin d'avoir une meilleure performance, le code étant très long à s'exécuter.
Mais tout ne s'est pas passé comme prévu : 
\begin{itemize}
    \item Lorsque mon code s'exécute sur l'instance, il s'arrête au bout de 5min avec une Exception provoquée par une réponse vide de Beautiful Soup.
    Je devais donc redémarrer l'instance afin que cela remarche, mais le problème revenait toujours, à une entreprise différente. En local, cela se produisait aussi, mais beaucoup plus tard.
    J'ai essayé de trouver une solution en cherchant un autre moyen de crawler, et suis tombée sur l'API Google Custom Search, qui fonctionnait beaucoup mieux avec des résultats fiables, mais qui avait un nombre de requêtes maximal par jour très bon pour la version gratuite.
    \item Le blocage du téléchargement que j'avais résolu en local, apparaissait sur l'instance de nouveau, ce qui bloquait l'ensemble du code.
\end{itemize}

J'ai donc organisé un court meeting afin de mettre l'équipe au courant de mes problèmes et de demander de l'aide.
Cette réunion m'a bien débloquée grâce aux solutions proposées :
En réalité, lorsque le code s'exécute rapidement, Beautiful Soup était "débordé" et n'avait pas toujours le temps de donner une réponse.
Avec un simple "time.sleep" de 0,8 secondes, le problème était résolu. La seule conséquence était que le temps d'exécution était un peu plus long.

Concernant le problème de téléchargement, j'ai mis en place un timeout et attrapé l'exception levée afin de continuer le code malgré tout. 
Les PDFs pour lesquels une exception a été levée sont sauvegardés dans un fichier afin de pouvoir les compter après-coup.
J'ai donc implémenté le calcul du pourcentage par an et du nombre total de PDFs n'ayant pas pu être téléchargés.
\\

Une journée de travail entière ne suffisant pas pour que le code s'exécute complètement, j'ai dû trouver une solution pour qu'il puisse reprendre là où il en etait lorsque l'instance se déconnecte ou que je dois fermer la page.

Parmi les différents changements à effectuer pour cela dans le programme, il fallait rendre les parties "web crawling" et "téléchargement/lecture des pdfs" complètement indépendantes l'une de l'autre.
Lors de la 1ère étape, au lieu de remplir des tableaux qui se vident à chaque nouvelle relance, il était pertinent de directement remplir les fichiers json (doubt-results0 ou found-results0) regroupant tous les liens trouvés ou non trouvés (ces fichiers n'étaient auparavent remplis qu'à la fin de chaque année parcourue). 
Ainsi à la 2ème étape, ces fichiers json pouvaient être lus pour trouver les liens et non dans les tables contenues dans les variables.
Ces liens étaient ensuite placés dans l'un des 2 nouveaux fichiers (doubt-results1 ou found-results1), au niveau de l'année courante.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{doubt_results_1.png}
    \caption{Extrait des résultats douteux après la lecture des pdfs (doubt-results-1.json)}
\end{figure}

La dernière entreprise traitée avec l'année en cours était notée dans un fichier txt lu à chaque nouvelle relance.
\\

En parcourant le fichier csv de la liste MSCI, j'ai remarqué que j'avais oublié beaucoup de termes interdits. 
Mais je devais à la fois vérifier que les rapports étaient toujours trouvés avec le nom de l'entreprise raccourci.
J'ai donc passé un petit temps à nettoyer les noms.
Un peu plus tard, j'ai remarqué que certaines entreprises avaient besoin des termes interdits pour être trouvées (Ex : Post holdings / software ag problem / fp corp).
J'ai donc modifié ma stratégie et ai instauré une liste de mots tolérés, et une de mots interdits. Ainsi, tous les termes après les mots tolérés sont supprimés (Ex : Tous ceux apres "inc").
Pour que la recherche du nom dans les rapports fonctionne toujours, j'ai mis en place la même règle que pour les liens : que seule une partie du nom peut être présente.
\\

J'ai donc pu tenter de lancer mon code pour de bon sur l'instance AWS, mais le fait d'y enregistrer les quelques 20 000 rapports dans le Repository Git posait problème, et plus aucune commande ne fonctionnait (git push, pull, add, status etc...).
Même en tentant de mettre en place git LFS, rien n'y faisait.
Afin de pouvoir accéder aux PDFs et étant donné que l'instance ne disposait que d'un terminal, j'ai pensé à uploader les PDFs directement sur Dropbox à la volée grâce à l'API, puisqu'ils devaient dans tous les cas y être à la fin.
Cela fonctionnait sur mon PC, mais pas sur l'instance, j'ai donc abandonné l'idée de l'utiliser. 
L'exécution était donc plus pratique et même plus réussie sur ma machine personnelle, le téléchargement étant moins souvent bloqué par les sites.

Alors que j'avais bien entamé l'exécution en local, j'ai remarqué que toutes les heures environ, le token expirait. Il fallait donc manuellement en regénérer un nouveau.
Il était donc finalement plus simple de d'enregistrer les PDFs en local et d'après coup tout uploader en même temps.
J'ai pu utiliser le forfait Business de Dropbox de mon tuteur pour y uploader les 136GB de PDFs.
\\

Les dernières observations que j'ai faites sur mes résultats, sont que :

1) Les résultats contenaient des doublons, soit à cause du fichier MSCI initial, soit quand je relançais le code, et qu'une entreprise qui avait déja été faite avant, soit refaite une 2ème fois.

2) qu'en fin de compte, le fait de considérer comme juste un lien qui contient les 2 derniers chiffres de l'année recherchée, faussait les résultats plus qu'autre chose :
certains liens contenaient juste une suite aléatoire de chiffres qui ne faisaient pas référence à l'année, et ils étaient considérés à tort comme justes.

J'ai donc supprimé cette condition, résolu le bug des doublons, et relancé le code une bonne fois pour toute, qui avait besoin de plus de 2 jours pour s'exécuter complètement.
\\

Mes fichiers statisitques contenaient désormais plus d'informations qu'au début : 
Pourcentage de PDFs trouvés pour une année spécifiques, nombre cumulé de PDFs douteux, trouvés, et à trouver au total :

\begin{figure}[H]
    \centering
    \includegraphics[width=7cm]{stats0.png}
    \caption{Statistiques de 2017 avant le téléchargement des PDFs}
\end{figure}

Pour finir, je devais faire une présentation de quelques slides afin de présenter mes résultats à l'équipe Gaia.
Un léger refactoring du code (division en plus de fonctions) et surtout sa documentation étaient également nécessaires. 
\\
Les résultats obtenus pour ce projet sont donc présentés sur les slides suivantes :

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{gaia_results.png}
    \caption{Résultats du projet GAIA (nombre de PDFS trouvés et douteux)}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=8cm]{pdfs_not_downloaded.png}
    \caption{Statistiques sur les PDFs n'ayant pu être téléchargés}
\end{figure}

Ces résultats étaient donc plutôt bons et ont satisfait l'équipe Gaia.

\pagebreak

\subsubsection{Projet ESCB Exchange}

Pour ce projet, ma mission était très courte et devait être réalisée en 8 jours maximum, et en 5 jours elle était terminée. 

Je devais donc reprendre le code écrit précédemment pour la comparaison bilatérale des sources de données, et mettre à jour la présentation existante avec les nouveaux résultats.

La fusion des 3 datasets sur le code ISIN des entreprises menait à un échantillion assez restreint d'observations, mais suffisant pour les représenter graphiquement.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{escb sample.png}
    \caption{Résultats de la fusion des datasets}
\end{figure}

Il fallait ensuite, pour chaque "Scope" (1, 2, 3) (= périmètres du bilan d'émissions de CO2) comparer les sources deux à deux graphiquement et obtenir le coefficient de correlation.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{escb graph.png}
    \caption{Comparaison des sources ISS et Carbon4Finance}
\end{figure}

Afin de pouvoir déterminer le dataset contenant les données les plus justes, on cherche pour chaque scope le nombre d'observations pour lesquelles, dans une marge de tolérance de 10\% : \\
- Les valeurs concordent toutes entre les trois sources de données \\
- Aucune source de données n'est d'accord avec les autres\\

Le reste des observations peuvent alors être comparées. Le datasource qui est le plus de fois en accord avec d'autres est le plus exact.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{escb agree.png}
    \caption{Nombre d'observations identiques entre les sources}
\end{figure}

La principale difficulté durant ce projet était de comprendre le code d'une autre personne portant sur des opérations statistiques ainsi que d'où venaient cetains résultats notés dans la présentation.

Mon tuteur m'a ensuite suggéré d'essayer la comparaison quadrilatérale, avec en plus les données d'ERICA, mais seulement 10 observations restaient après la fusion. 
Les données plus récentes sont donc attendues.

\pagebreak

\subsubsection{Projet CSDB}

Ma mission pour ce projet est donc de trouver une méthode de comparaison de datasets optimale et efficace.

J'ai dans un premier temps cherché les différentes méthodes qui existent en Python pour comparer facilement des datasets.
Il y a notamment les méthodes "compare" et "equals" de pandas, mais celle-ci ne fonctionne que sur des datasets de forme identique.
Cela n'était donc pas garanti, par exemple lorsque des colonnes étaient ajoutées sur une nouvelle version.

La package Datacompy en revanche, semblait remplir la mission à merveille, avec la possibilité de générer un rapport détaillé sur les différences et similitudes trouvées.

J'ai d'abord testé ces packages sur des petits Dataframes de test.

Le seul problème était que les lignes étaient vues comme différentes lorsqu'elles contiennent "" ou " ", de même lorsqu'un dataframe contient une colonne supplémentaire mais qui est vide (On veut qu'ils soient considérés dans ce cas comme identiques).
Une fois ces détails résolus, j'ai pu tester la méthode de comparaison sur les premiers vrais fichiers csv, pesant plus de 2Go.
\\

A la première exécution, je remarque que toutes les lignes sont différentes. Il fallait en fait trier les datasets par ISIN afin que la comparaison se fasse correctement.
IL restait tout de même des fausses différences invisibles à l'oeil nu, à cause d'un nombre différent d'espaces (pouvant aller jusqu'à 150 espaces). Ceux-ci sont donc remplacés par None.
Parfois des espaces à la fin des mots étaient aussi présents, il était donc nécessaire de les supprimer. 

Concernant les nombres, ils pouvaient souvent arriver qu'ils soient arrondis seulement dans l'un des datasets.
J'ai résolu cela en comparant les floats avec une tolérence de 1\%.
Pour 1 colonne, cette méthode n'a pas fonctionné, peut être à cause d'un format string. 
Ici, les mêmes entiers étaient succédés par un ".0" dans l'un des datasets.
Après de longues recherches, je n'ai finalement pas trouvé de solution, celles trouvées cassant toutes les valeurs de toutes les colonnes avec un nombre inifini de "0".

Le dernier obstacle était les dates qui n'étaient pas du même format dans tous les datasets, étant parfois notés en toutes lettres.
Les colonnes de dates ont finalement été toutes supprimées, n'étant pas importantes.

Au fil de l'utilisation du package Datacompy, j'ai découvert que l'on pouvait joindre les datasets sur une colonne spécifique, rendant le tri inutile.
\\

Cependant, nous avons soupçonné que l'autre méthode de comparaison, un peu plus artisanale, serait plus optimale.
En effet, il ne nécessite que très peu de lignes de code, mais chaque ligne est très longue à s'exécuter, calculant beaucoup d'informations non indispensables pour nous (20min en tout).

J'ai donc tenté de comparer les colonnes une à une avec les méthodes de pandas, et il s'est avéré que cette solution était beaucoup plus efficace (8min vs 20min d'exécution).
Les colonnes supplémentaires étaient ensuite recherchées puis inspectées afin de voir si elles sont vides.

Tout fonctionnait parfaitement, à part ce problème de ".0" et pour certaines chaînes de caractères encodées différemment.
\\

Pour terminer, j'ai exécuter mon code pour les 12 mois de 2015, et documenté les résultats dans un markdown.
Pour ce faire, j'ai dû utiliser un Virtual Computer, car ces datasets faisaient entre 4 et 7Go.

\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{csdb results.png}
    \caption{Extrait des résultats - Premiers datasets - avril 2009}
\end{figure}

Après cela, il m'a été conseillé de me concentrer sur le nouveau projet d'NLP, afin de ne pas avoir la tête sur plusieurs projets en même temps.

\pagebreak

\subsubsection{Projet NFIG}

Tout comme le projet CSDB Exchange, celui-ci n'a duré que 5 jours pour la génération du rapport.

Avant cela, j'avais dû essayer de comprendre le code déjà écrit pour trouver automatiquement les fichiers générés dans le code, au moment où nous croyions encore avec mon collègue qua nous allions travailler là-dessus.

2 semaines plus tard, ma vraie mission a pu commencer : 

A partir de 2 fichiers Excel comme ci-dessous, regroupant quel fichier output est généré à quelle ligne dans quel fichier .do et .log de quelle date, je devais donc générer un rapport avec Python selon le modèle de la figure 16.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{nfig excel.png}
    \caption{Fichier Excel à partir duquel le rapport devait être généré}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{nfig exemple rapport.png}
    \caption{Exemple de rapport type à générer automatiquement}
\end{figure}

Comme le modèle de rapport le montre, la 1ère colonne du tableau est elle-même composée de 2 colonnes, ce qui impliquait d'avoir des dataframes emboîtés.
Cela a été un peu compliqué à mettre en place, mais a finalement bien fonctionné à la fin, en choisissant un format HTML plutôt que Markdown pour le rapport.

Egalement, la colonne "n" fait référence à la colonne "freq" dans l'Excel, mais il n'était pas clair si ce nombre n'était valable que pour 1 apparition (autrement dit, 3 apparitions dans 1 fichier = 3 fréquences), par exemple si le nom du fichier output apparait plusieurs fois à la même ligne; 
ou si une fréquence était valable pour le nombre total des apparitions, comme le montre le modèle.

Dans le doute, j'ai choisi la 1ère option, plus compliquée à implémenter, et le résultat ressemble à cela :

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{nfig rapport.png}
    \caption{Extrait du rapport généré automatiquement}
\end{figure}

J'ai demandé au client ce qu'il en était pour ce doute, il devait lui-même vérifier mais ne m'a jamais redit.

\subsubsection{Projet de recherche NLP}

Pour commencer tout travail de recherche, il faut débuter par la revue littéraire en anglais pour trouver des articles similaires. 
J'en ai donc trouvé 8 qui se rapporchaient de près ou de loin à notre sujet, soit recherchant juste des datasets mentionnés dans les papiers de recherche, soit les méthodologies, ou utilisant un LLM.
Ce que nous souhaiterions dans l'idéal atteindre est de trouver seulement les datasets et méthodologies utilisées, et non simplement mentionnées.
\\

La 2nde tâche était de diviser les papiers de recherche en phrases, afin de pouvoir plus tard les fournir une à une à ChatGPT. 
Pour cela, j'ai utilisé le package de NLP nltk, mais ai dû ajouter à la main quelques règles, notamment pour "et al." et "N." pour numéro, où les points étaient là considérés comme des fins de phrases.
J'ai alors dû mettre en place une fonction récursive, ce qui était assez délicat mais très formateur.
Installer le package NLTK n'était également pas des plus simple à la Bundesbank, certaines dépendances devaient être téléchargées à la main, dû aux diverses autorisations.
\\

Une fois cela terminé, ma mission était de libeller quelques phrases contenant des mentions de datasets et d'autres n'en contenant pas, puis de fournir ces phrases à ChatGPT et d'observer les résultats.
Concernant la question exacte à poser à ChatGPT, il était plus efficace de lui donner la définition d'un dataset ou d'une méthodologie, puis de lui demander de s'appuyer sur cette définition pour trouver les éléments.
Cependant, toute suite de nombres était considérée comme un dataset, or cela ne nous intéressait pas, souhaitant seulement les noms des datasets utilisés, et non ceux fabriqués durant la recherche.

Un autre département de la Bundesbank (IT7) était intéressé par le projet et lors de la réunion avec eux, nous avons pu présenter ces résultats.
Pour des raisons budgétaires, il était finalement préférable selon eux de diviser les papiers en pages plutôt qu'en phrases afin de pas devoir fait trop de petites requêtes.

J'ai donc de nouveau sélectionné des pages contenant des datasets ou méthodologies, et d'autres ne contenant rien, documenté le tout dans un Excel, et envoyé le fichier à l'IT7 afin qu'ils piussent tester les échantillons facilement eux-même.
\\

L'intention de cette recherche devait être officiellement déposée à un comité de recherche, il fallait donc soumettre l'Abstract assez tôt. 
Une collègue l'a écrit, et le reste de l'équipe devait le relire et corriger, ce à quoi j'ai participé.
\\

Cette même collègue a aussi commencé l'écriture d'un code visant à filtrer les papiers pour supprimer les parties non pertinentes, notamment les pages Titre, les Références, etc\dots
Comme j'étais censée mettre en commun nos 2 codes pour préparer des extraits directement utilisables pour ChatGPT mais qu'il n'était pas terminé, j'ai décidé de le faire moi-même.
Elle avait, dans un Jupyter Notebook, préparer les différentes sections pour le code à venir, par exemple "Suppression de l'Introduction", "Division en sections", etc\dots
\\

A ce moment s'est présentée une phase de réflexion sur ce que nous souhaitions vraiment, et s'il était vraiment pertinent de supprimer autant de parties.
En me renseignant auprès de Sebastian, mon référant technique pour ce projet, j'ai appris que les datasets se référant à un sondage conduit pour la recherche comptent, ainsi que les parties de datasets utilisées (Ex: La 1ère vague de tel dataset).
Plus spécifiquement, j'ai appris que l'Introduction ne devait pas être supprimée, ni les descriptions de Tableaux ou Figures.

Dans un premier temps, j'ai tenté de trouver un moyen de supprimer les tableaux de fichiers PDF.
Certains packages Python existaient pour extraire les tableaux en Dataframes, mais ne permettaient pas de les supprimer du fichier, et surtout ne reconnaissaient pas les tableaux contenant seulement des bordures de lignes et non de colonnes.
D'autres packages ayant l'air intéressant ne pouvaient simplement pas être installés à la Bundesbank, recquérant trop de dépendances ou autres logiciels (ex: Anaconda).

Après de longues recherches, j'ai conclu qu'il était nécessaire d'obtenir un format structuré à partir des PDF.

J'ai tenté de trouver un convertisseur Markdown, mais aucun n'existait pour Python, sauf payant.
J'ai ensuite essayé de convertir en HTML, mais les balises résultantes étaient toutes des <span>, y compris pour les tableaux.
\\

Finalement, ma solution était de convertir les PDF en DOCX (package pdf2docx). 
Les tableaux y étaient après directement détectables, et grâce au package docx, il était possible de faire toutes sortes de modifications sur les docx, y compris supprimer les tableaux.
De nombreuses possibilités s'ouvraient alors : il était désormais possible de trouver les caractères en gras (toujours des titres) et de supprimer le texte en 2 titres.
J'ai donc pu supprimer la section Références beaucoup plus efficacement et en moins de lignes que ce qu'avait précédemment fait ma collègue.
Le lien présent en bas de chaque a également pu être ainsi supprimé.
A la fin, le docx pouvait être reconvertir en PDF grâce au package docx2pdf.
La conversion en docx n'a simplement pas fonctionné sur 1 pdf sur 6. J'ai donc aussi traité les exceptions.

Pour ce qui est de la suppression de la page titre, j'ai simplement repris le code de ma collègue qui fonctionnait sur les papiers en format txt découpés en pages grâce à Fitz, car le format docx ne reconnait pas les numéros de pages. 
Il y a donc 2 phases de filtrage.
\\

Sebastian a en parallèle écrit un code visant à communiquer avec l'API de ChatGPT qui lui envoie les extraits de textes et enregistre les réponses dans des txt, ce qui est très utile pour automatiser les requêtes et avoir accès à plus d'options.
La prochaine étape était donc l'incorporation de mon filtre au processus de questionnement de ChatGPT.
J'ai pu dans ce contexte apprendre à utiliser les classes en Python.
Une difficulté que j'ai rencontrée était de comprendre le code déja fait et de s'y adapter le mieux possible pour incorporer proprement mon code.

\subsection{Planning général suivi}